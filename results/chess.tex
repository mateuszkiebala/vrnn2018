\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete egpaper.aux
% before re-running latex.  (Or just hit 'q' on the first latex run, let it
% finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{VRNN 2018 - Predicting legal chess moves \\ \large {\small "Chess is the struggle against the error."} }


\author{Mateusz Kiebala\\
University of Warsaw\\
{\tt\small mk359758@students.mimuw.edu.pl}
% For a paper whose authors are all at the same institution, omit the following
% lines up until the closing ``}''. Additional authors and addresses can be
% added with ``\and'', just like the second author. To save space, use either
% the email address or home page, not both
\and
Tomasz Knopik\\
University of Warsaw\\
{\tt\small tk359778@students.mimuw.edu.pl}
\and
Janusz Marcinkiewicz\\
University of Warsaw\\
{\tt\small jm360338@students.mimuw.edu.pl}}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   One of the recent problems that gained a lot of attention in Deep Learning
   field was playing Go and Chess at the professional level. The models for
   those problems are assigning the probability of the win for certain moves and
   positions where the moves are generated deterministically. In our work, we
   are checking if it is possible to learn a CNN network the rules of chess
   providing the only image of the board before and after the move. We test two
   different models architectures: single CNN model which processes two input
   images as one concatenated; and Siamese CNN model which takes two images
   separately, extracts features and combines them together predicting the
   result. Those two approaches achieve $\thicksim 90\%$ on the training set and
   $\thicksim 86\%$ on the validation set.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Recent achievements in Deep Learning allowed researchers to beat top players in
Go and Chess using Deep Learning models\cite{silver2017mastering}. To learn such
model authors have used techniques like Reinforcement Learning and Deep
Convolutional Neural Networks to predict win percentages for given positions.
Those models relay on deterministic move generation and known encoding of
position.

In our work, we wanted to change the perspective of playing chess. We wanted our
model to learn chess rules without prior definition of what they are and what is
the purpose of the game. This is exactly what people, who did not see games like
Chess or Go before, do when they see two other players playing those games --
they try to understand the possibilities, rules and purpose of the game just by
looking at how the players move.

We believe that this problem is super important. If the model can learn the
rules of chess, maybe it also can learn the rules of the world. Long-standing
question about single formula describing the world could be solved by CNN. It is
still long way ahead of us but we believe we can make it and our work is the
first step towards this achievement.

\section{Related work}

We have struggled to find any papers or work done in this area. All the papers
we have found, are targeting learning model to evaluate positions but not how to
play the game. In end-to-end chess playing paper~\cite{DeepChess} authors
discuss a model to not only evaluate positions but also determining if the move
is correct. We have used the key insights from the paper to improve our models'
architectures. Another paper about predicting chess
moves~\cite{Oshri2015PredictingMI} provided us with a lot of useful information
about using and not using different types of layers in our models.

\section{Data}

Chess is a deterministic game and the moves as well as positions can be easily
generated and encoded.

Data used in our experiments was generated using python chess
library\footnote{\url{https://github.com/niklasf/python-chess}} which allowed to
save given position as images which then we converted into numpy
arrays\footnote{\url{http://www.numpy.org}}. We have created a framework for
generating random data with different outputs which were necessary for different
types of models we have tested. The data was generated from random plays as well
as positions and moves from book
games\footnote{\url{https://www.ficsgames.org/download.html}}. We have combined
these two sources into datasets on which our models were trained and tested.
This way we wanted to prevent our model from overfitting on specific types of
plays. Wrong moves were created randomly by putting pieces on random squares on
currently processed chess board, and ensuring that such a move is not legal. The
ratio of legal and illegal moves was around $50$--$50$. What's worth noting is
the fact that introducing additional illegal moves, which fallacy was due to
checks or checkmates, had minor influence overall accuracy.

To train the models we have generated around $15$ datasets with different random
vs book games ratio each of them containing about $5'000$ moves. In total, we
gained about $70'000$ moves to train on.

\section{Methods}

In our approach, we tried two different types of CNN models.

\subsection{Single CNN model}

One of the first models we trained is CNN model which consists of couple
convolution layers followed by the fully connected layers\footnote{Single model
architecture:
\url{http://students.mimuw.edu.pl/~jm360338/vrnn2018/model_single.png}}. The
model, as an input, was fed with concatenated images: before and after a move.
In this model, we wanted to check if convolution layers can extract more
features having more information.

\subsection{Siamese CNN model}

Siamese CNN model consists of two identical CNN networks which output was then
concatenated and passed to the fully connected layers\footnote{Dual model
architecture:
\url{http://students.mimuw.edu.pl/~jm360338/vrnn2018/model_dual.png}}. This
model was fed with two inputs: image before and after a move. Our understanding
was that the model should independently extract features of each of the image
and in the output have some kind of encoding which then was fed to fully
connected layers.

\subsection{Deterministic encoding}

In our last method we tried to see what accuracy can we achieve if we pass
encoding of the chess board to fully connected layer without the need of
extracting features by convolution layers. This experiment was to see what is
the bottleneck: too shallow representation of features extracted by convolution
layers or fully connected layers. A term deterministic encoding means a chess
board, encoded as $\{0,1\}$ vector of length $768$ -- there are $12$ different
pieces (number of colors * number of pieces) each of them can occupy one of $64$
positions.

\subsection{Color vs grayscale}

In the beginning, we used colorful images ($3$ channels). We suspected that the
networks could have been learning specific features depending on color. Thus, we
started generating images in grayscale ($1$ channel). Unfortunately, we did not
observe any improvement and thus came to the conclusion that our convolution
layers must have been generalizing quite well.

\section{Experiments}

In the experiments, we have tried different approaches for different types of
models. In both types of models as well as deterministic encoding we achieved
$\thicksim 86\%$ on a validation set.

What may be interesting is that the deterministic encoding had similar accuracy
as the other models what suggest that convolution layers did a great job in
extracting features and the bottleneck in the accuracy was caused by fully
connected layers. Worth noting is the fact, that thanks to the compact encoding
representation, we were able to train a network on a much bigger dataset of
roughly $200'000$ moves. However, we observed that it did not have a significant
impact on results, as final accuracy reached $\thicksim 87\%$. With this
information, we tried to implement a different set of fully connected layers but
without further success. We decided to not go further with this part of a
project, as this is out of the course's scope.

\subsection{Hardware}

We trained our models on single 1080Ti GPU. Each of the models was trained on
$\thicksim 5000$ epochs with $64$ batch size. Single dataset was trained for
$10$ epochs, after that new dataset was loaded and it was trained for $10$
epochs as well, then whole process repeated. As we aformentioned it before, we
have generated $15$ different datasets -- each of them contained new images and
some of them had different random/book games ratio. Therefore, each of the
datasets was trained around $30$ times.

\subsection{Method comparison}

\begin{center}
    \begin{tabular}{ | p{5em} | p{5em} | p{5em} | p{5em} |}
    \hline
    Method & Dataset size & No. epochs & Accuracy \\ [0.5ex] \hline \hline
    Single & $15'000$ & $10'000$ & $85.2\%$ \\
    \hline
    Siamese & $70'000$ & $4'000$ & $86.2\%$ \\ \hline
    Deterministic encoding & $200'000$ & $10'000$ & $87\%$ \\
    \hline
    \end{tabular}
\end{center}

\section{Conclusions}

At the very beginning of the project, our goal was to solve a problem of pattern
recognition and logical reasoning based on rules of chess. We did not accomplish
any significant results taking into consideration solely the given task.
However, our experiments turned out to be successful in terms of image
recognition problem.

Our project contributed to show that logical reasoning is a hard problem even if
it is limited to a specific use case. It shows that well-crafted convolution
neutral network successfully extracts desired features from images, even when a
loss function describes an abstract concept, like the legality of chess move,
rather than a simple idea like a class of an object on an image. The image
recognition overhead is relatively small ($\thicksim 1 \text{-} 2$ percentage
points), compared to the perfect representation of deterministic encoding.

There are plenty of opportunities for further work. They include, but are not
limited to:
\begin{itemize}
\item testing proposed solutions on better hardware and much larger datasets, as
we provide a convenient way to generate an almost unlimited dataset,
\item evaluating our CNN solution with better-designed neutral network for
logical reasoning part.
\end{itemize}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
